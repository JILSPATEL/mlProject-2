{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) - Data Preprocessing\n",
    "## Forest Cover Type Dataset\n",
    "\n",
    "This notebook prepares data for SVM modeling. SVMs are distance-based algorithms, so proper feature scaling is critical for good performance.\n",
    "\n",
    "### Preprocessing Steps:\n",
    "1. Load raw dataset\n",
    "2. Split into training and test sets (stratified)\n",
    "3. Apply standard scaling (essential for SVM)\n",
    "4. Save processed data and scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:27.953949Z",
     "iopub.status.busy": "2025-12-12T12:48:27.953949Z",
     "iopub.status.idle": "2025-12-12T12:48:28.667737Z",
     "shell.execute_reply": "2025-12-12T12:48:28.667737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:28.691866Z",
     "iopub.status.busy": "2025-12-12T12:48:28.691866Z",
     "iopub.status.idle": "2025-12-12T12:48:28.695607Z",
     "shell.execute_reply": "2025-12-12T12:48:28.695251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SVM PREPROCESSING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SVM PREPROCESSING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Locate Dataset\n",
    "\n",
    "Robust path handling to find the covtype.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:28.696643Z",
     "iopub.status.busy": "2025-12-12T12:48:28.696643Z",
     "iopub.status.idle": "2025-12-12T12:48:28.701107Z",
     "shell.execute_reply": "2025-12-12T12:48:28.700588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found dataset at: C:\\PYTHON\\AIT511 Course Project 2\\archive\\covtype.csv\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.abspath('../..')\n",
    "parent_dir = os.path.dirname(script_dir)\n",
    "\n",
    "possible_paths = [\n",
    "    os.path.join(script_dir, 'covtype.csv'),\n",
    "    os.path.join(parent_dir, 'covtype.csv'),\n",
    "    'covtype.csv',\n",
    "    '../covtype.csv',\n",
    "    '../../covtype.csv'\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        csv_path = path\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    print(\"Error: covtype.csv not found!\")\n",
    "    print(\"Checked paths:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"  - {path}\")\n",
    "    raise FileNotFoundError(\"covtype.csv not found\")\n",
    "\n",
    "print(f\"✓ Found dataset at: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:28.702140Z",
     "iopub.status.busy": "2025-12-12T12:48:28.702140Z",
     "iopub.status.idle": "2025-12-12T12:48:29.507488Z",
     "shell.execute_reply": "2025-12-12T12:48:29.507488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded: (581012, 55)\n",
      "  - Rows: 581,012\n",
      "  - Columns: 55\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 581012 entries, 0 to 581011\n",
      "Data columns (total 55 columns):\n",
      " #   Column                              Non-Null Count   Dtype\n",
      "---  ------                              --------------   -----\n",
      " 0   Elevation                           581012 non-null  int64\n",
      " 1   Aspect                              581012 non-null  int64\n",
      " 2   Slope                               581012 non-null  int64\n",
      " 3   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n",
      " 4   Vertical_Distance_To_Hydrology      581012 non-null  int64\n",
      " 5   Horizontal_Distance_To_Roadways     581012 non-null  int64\n",
      " 6   Hillshade_9am                       581012 non-null  int64\n",
      " 7   Hillshade_Noon                      581012 non-null  int64\n",
      " 8   Hillshade_3pm                       581012 non-null  int64\n",
      " 9   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n",
      " 10  Wilderness_Area1                    581012 non-null  int64\n",
      " 11  Wilderness_Area2                    581012 non-null  int64\n",
      " 12  Wilderness_Area3                    581012 non-null  int64\n",
      " 13  Wilderness_Area4                    581012 non-null  int64\n",
      " 14  Soil_Type1                          581012 non-null  int64\n",
      " 15  Soil_Type2                          581012 non-null  int64\n",
      " 16  Soil_Type3                          581012 non-null  int64\n",
      " 17  Soil_Type4                          581012 non-null  int64\n",
      " 18  Soil_Type5                          581012 non-null  int64\n",
      " 19  Soil_Type6                          581012 non-null  int64\n",
      " 20  Soil_Type7                          581012 non-null  int64\n",
      " 21  Soil_Type8                          581012 non-null  int64\n",
      " 22  Soil_Type9                          581012 non-null  int64\n",
      " 23  Soil_Type10                         581012 non-null  int64\n",
      " 24  Soil_Type11                         581012 non-null  int64\n",
      " 25  Soil_Type12                         581012 non-null  int64\n",
      " 26  Soil_Type13                         581012 non-null  int64\n",
      " 27  Soil_Type14                         581012 non-null  int64\n",
      " 28  Soil_Type15                         581012 non-null  int64\n",
      " 29  Soil_Type16                         581012 non-null  int64\n",
      " 30  Soil_Type17                         581012 non-null  int64\n",
      " 31  Soil_Type18                         581012 non-null  int64\n",
      " 32  Soil_Type19                         581012 non-null  int64\n",
      " 33  Soil_Type20                         581012 non-null  int64\n",
      " 34  Soil_Type21                         581012 non-null  int64\n",
      " 35  Soil_Type22                         581012 non-null  int64\n",
      " 36  Soil_Type23                         581012 non-null  int64\n",
      " 37  Soil_Type24                         581012 non-null  int64\n",
      " 38  Soil_Type25                         581012 non-null  int64\n",
      " 39  Soil_Type26                         581012 non-null  int64\n",
      " 40  Soil_Type27                         581012 non-null  int64\n",
      " 41  Soil_Type28                         581012 non-null  int64\n",
      " 42  Soil_Type29                         581012 non-null  int64\n",
      " 43  Soil_Type30                         581012 non-null  int64\n",
      " 44  Soil_Type31                         581012 non-null  int64\n",
      " 45  Soil_Type32                         581012 non-null  int64\n",
      " 46  Soil_Type33                         581012 non-null  int64\n",
      " 47  Soil_Type34                         581012 non-null  int64\n",
      " 48  Soil_Type35                         581012 non-null  int64\n",
      " 49  Soil_Type36                         581012 non-null  int64\n",
      " 50  Soil_Type37                         581012 non-null  int64\n",
      " 51  Soil_Type38                         581012 non-null  int64\n",
      " 52  Soil_Type39                         581012 non-null  int64\n",
      " 53  Soil_Type40                         581012 non-null  int64\n",
      " 54  Cover_Type                          581012 non-null  int64\n",
      "dtypes: int64(55)\n",
      "memory usage: 243.8 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/5] Loading dataset...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"✓ Dataset loaded: {df.shape}\")\n",
    "print(f\"  - Rows: {df.shape[0]:,}\")\n",
    "print(f\"  - Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Features and Target\n",
    "\n",
    "### SVM Preprocessing Strategy:\n",
    "1. **Feature Selection**: Use all features initially (54 dimensions is manageable)\n",
    "2. **Scaling**: MUST standardize everything (SVM is distance-based)\n",
    "3. **Dimensionality Reduction**: Optional - LinearSVC handles 54 dims efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:29.509526Z",
     "iopub.status.busy": "2025-12-12T12:48:29.509526Z",
     "iopub.status.idle": "2025-12-12T12:48:29.531616Z",
     "shell.execute_reply": "2025-12-12T12:48:29.531103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (581012, 54)\n",
      "Target shape: (581012,)\n",
      "\n",
      "Target classes: [1 2 3 4 5 6 7]\n",
      "Number of classes: 7\n",
      "\n",
      "Class distribution:\n",
      "  Class 1: 211,840 (36.46%)\n",
      "  Class 2: 283,301 (48.76%)\n",
      "  Class 3: 35,754 ( 6.15%)\n",
      "  Class 4:  2,747 ( 0.47%)\n",
      "  Class 5:  9,493 ( 1.63%)\n",
      "  Class 6: 17,367 ( 2.99%)\n",
      "  Class 7: 20,510 ( 3.53%)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nTarget classes: {np.unique(y)}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"\\nClass distribution:\")\n",
    "for cls, count in zip(unique, counts):\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"  Class {cls}: {count:6,} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split Data\n",
    "\n",
    "Using stratified split to maintain class distribution in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:29.533621Z",
     "iopub.status.busy": "2025-12-12T12:48:29.533621Z",
     "iopub.status.idle": "2025-12-12T12:48:30.033761Z",
     "shell.execute_reply": "2025-12-12T12:48:30.033761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Splitting data (80% train, 20% test)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data split complete\n",
      "  - Training samples: 464,809\n",
      "  - Test samples: 116,203\n",
      "  - Features: 54\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/5] Splitting data (80% train, 20% test)...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"✓ Data split complete\")\n",
    "print(f\"  - Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"  - Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"  - Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling\n",
    "\n",
    "**Critical for SVM**: StandardScaler transforms features to have mean=0 and variance=1.\n",
    "\n",
    "This ensures that:\n",
    "- All features contribute equally to distance calculations\n",
    "- The algorithm converges faster\n",
    "- Performance is significantly improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:30.036069Z",
     "iopub.status.busy": "2025-12-12T12:48:30.035070Z",
     "iopub.status.idle": "2025-12-12T12:48:30.560738Z",
     "shell.execute_reply": "2025-12-12T12:48:30.560738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Applying Standard Scaling...\n",
      "(Essential for SVM - distance-based algorithm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data scaled (mean=0, variance=1)\n",
      "\n",
      "Scaling verification (training set):\n",
      "  - Mean: -0.000000 (should be ~0)\n",
      "  - Std: 1.000000 (should be ~1)\n",
      "  - Min: -11.296430\n",
      "  - Max: 393.618258\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/5] Applying Standard Scaling...\")\n",
    "print(\"(Essential for SVM - distance-based algorithm)\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✓ Data scaled (mean=0, variance=1)\")\n",
    "\n",
    "print(\"\\nScaling verification (training set):\")\n",
    "print(f\"  - Mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"  - Std: {X_train_scaled.std():.6f} (should be ~1)\")\n",
    "print(f\"  - Min: {X_train_scaled.min():.6f}\")\n",
    "print(f\"  - Max: {X_train_scaled.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Statistics Comparison\n",
    "\n",
    "Compare feature statistics before and after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:30.562742Z",
     "iopub.status.busy": "2025-12-12T12:48:30.562742Z",
     "iopub.status.idle": "2025-12-12T12:48:30.664688Z",
     "shell.execute_reply": "2025-12-12T12:48:30.664442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature statistics comparison (first 5 features):\n",
      "\n",
      "Before scaling:\n",
      "  Mean: [2959.51065922  155.82856399   14.10456123  269.34836029   46.42075347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Std:  [280.02477598 111.97966415   7.48716583 212.38921881  58.23262633]\n",
      "\n",
      "After scaling:\n",
      "  Mean: [-3.73055182e-16  5.45403220e-17 -1.37429717e-15  1.11315952e-15\n",
      " -7.12673902e-18]\n",
      "  Std:  [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeature statistics comparison (first 5 features):\")\n",
    "print(\"\\nBefore scaling:\")\n",
    "print(f\"  Mean: {X_train[:, :5].mean(axis=0)}\")\n",
    "print(f\"  Std:  {X_train[:, :5].std(axis=0)}\")\n",
    "\n",
    "print(\"\\nAfter scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled[:, :5].mean(axis=0)}\")\n",
    "print(f\"  Std:  {X_train_scaled[:, :5].std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dimensionality Note\n",
    "\n",
    "With 54 features, we don't need PCA for dimensionality reduction. LinearSVC scales linearly with dimensions, making it efficient for this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:30.666695Z",
     "iopub.status.busy": "2025-12-12T12:48:30.665725Z",
     "iopub.status.idle": "2025-12-12T12:48:30.669127Z",
     "shell.execute_reply": "2025-12-12T12:48:30.669127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensionality: 54 features\n",
      "Note: LinearSVC handles 54 dimensions efficiently.\n",
      "PCA not required for this dataset size.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDimensionality: {X_train_scaled.shape[1]} features\")\n",
    "print(\"Note: LinearSVC handles 54 dimensions efficiently.\")\n",
    "print(\"PCA not required for this dataset size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data\n",
    "\n",
    "Save both the processed data and the fitted scaler for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:30.671182Z",
     "iopub.status.busy": "2025-12-12T12:48:30.670134Z",
     "iopub.status.idle": "2025-12-12T12:48:32.754454Z",
     "shell.execute_reply": "2025-12-12T12:48:32.754454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] Saving processed data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved processed data to: C:\\PYTHON\\AIT511 Course Project 2\\archive\\svm_implementation\\data\\svm_data.npz\n",
      "✓ Saved scaler to: C:\\PYTHON\\AIT511 Course Project 2\\archive\\svm_implementation\\data\\svm_scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/5] Saving processed data...\")\n",
    "\n",
    "output_dir = os.path.join(script_dir, 'svm_implementation', 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "data_file = os.path.join(output_dir, 'svm_data.npz')\n",
    "np.savez_compressed(\n",
    "    data_file,\n",
    "    X_train=X_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "scaler_file = os.path.join(output_dir, 'svm_scaler.joblib')\n",
    "joblib.dump(scaler, scaler_file)\n",
    "\n",
    "print(f\"✓ Saved processed data to: {data_file}\")\n",
    "print(f\"✓ Saved scaler to: {scaler_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T12:48:32.756498Z",
     "iopub.status.busy": "2025-12-12T12:48:32.755460Z",
     "iopub.status.idle": "2025-12-12T12:48:32.765572Z",
     "shell.execute_reply": "2025-12-12T12:48:32.765572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SVM PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "✓ Total samples: 581,012\n",
      "✓ Training samples: 464,809\n",
      "✓ Test samples: 116,203\n",
      "✓ Features: 54\n",
      "✓ Classes: 7\n",
      "✓ Scaling: StandardScaler (mean=0, std=1)\n",
      "\n",
      "Data ready for SVM training!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SVM PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Total samples: {len(y):,}\")\n",
    "print(f\"✓ Training samples: {len(y_train):,}\")\n",
    "print(f\"✓ Test samples: {len(y_test):,}\")\n",
    "print(f\"✓ Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"✓ Classes: {len(np.unique(y))}\")\n",
    "print(f\"✓ Scaling: StandardScaler (mean=0, std=1)\")\n",
    "print(f\"\\nData ready for SVM training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}